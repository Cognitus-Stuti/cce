{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_IaoW-XNbj5l"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_consecutive_entities(data):\n",
    "    entities = data\n",
    "    merged_entities = []\n",
    "    current_entity = entities[0]\n",
    "    for next_entity in entities[1:]:\n",
    "        if current_entity[\"label\"] == next_entity[\"label\"] :\n",
    "            if current_entity[\"end_offset\"] == next_entity[\"start_offset\"] or current_entity[\"start_offset\"] == next_entity[\"end_offset\"] or current_entity[\"end_offset\"]+1 == next_entity[\"start_offset\"] or current_entity[\"start_offset\"]+1 == next_entity[\"end_offset\"]:\n",
    "                current_entity[\"start_offset\"] = min(current_entity[\"start_offset\"], next_entity[\"start_offset\"])\n",
    "                current_entity[\"end_offset\"] = max(current_entity[\"end_offset\"], next_entity[\"end_offset\"])\n",
    "            else:\n",
    "                merged_entities.append(current_entity)\n",
    "                current_entity = next_entity\n",
    "        else:\n",
    "\n",
    "            merged_entities.append(current_entity)\n",
    "            current_entity = next_entity\n",
    "\n",
    "    merged_entities.append(current_entity)\n",
    "    return merged_entities\n",
    "\n",
    "\n",
    "input_file_path = \"all.jsonl\"\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        data.pop(\"relations\", None)  # Remove \"relations\" key if present\n",
    "        data.pop(\"Comments\", None)\n",
    "        data[\"entities\"].sort(key=lambda x: x[\"id\"])\n",
    "        if len(data[\"entities\"]) == 0:\n",
    "            continue\n",
    "        data[\"entities\"]= merge_consecutive_entities(data[\"entities\"])\n",
    "        merged_data.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RXCLoujpbj5m"
   },
   "outputs": [],
   "source": [
    "unique_labels = set(entity[\"label\"] for sample in merged_data for entity in sample[\"entities\"])\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "for sample in merged_data:\n",
    "    for entity in sample[\"entities\"]:\n",
    "        entity[\"label_id\"] = label2id.get(entity[\"label\"], -1)  # -1 for unknown labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234,
     "referenced_widgets": [
      "a7a3a3cf85594431bcd4083fc37f27b0",
      "aefbd8b096284a7c881843898e0b8f7d",
      "45531ed0f4304a2ca450da91ad56da99",
      "2beb325339134407a0f3fbd628fc5b7d",
      "1106cb7819864930b85ae9ed2fa77b75",
      "39d279c80b144d5cb072077b5d151184",
      "bbf34d63e45e4d3690f3d9c8e2377f1d",
      "cd780bc50b9d42f0ba5cd33cf65de99d",
      "6231388fd8cf42e29b5ccc5c4b092db1",
      "c361d449746f4c1ba24919cd545a771b",
      "e699a844118d447e9250ea8937b72901",
      "dff159194ad64b828cd60cc1609e854b",
      "c520a463f5dc4c0296dc2db826639311",
      "d4403d7387e4407ab5a15f13d85322a4",
      "fd1bd3ccf11a435ea54d2b4acfac68c6",
      "6632637f75274542ba4e823859b4f539",
      "cb2cffeda7f4434a9e1bb49ff730d163",
      "8dad636dd9e04a82b748b37bc6935168",
      "4beaf512f235458ea71740e8c392813d",
      "3792af996c4340a282e42b14a9efb283",
      "8898bab3099a4211b4807841075882f2",
      "37b3a7aac2d84314a4ea272ddd8901bb",
      "14e36d7103b144ff84079b1d24e72bb3",
      "4b6a8790d96f464fbc66fe2920c5ab97",
      "7fedd97ddc8c470dad68099dea2673ac",
      "ff94c67363a6420293100fa35d6a4996",
      "1e02bb5907b7407a94fca110a8b8949d",
      "e104cc2037b944b282c648f4baf3d01b",
      "987c8d8d33e84d77aa184be479c03c68",
      "fb6bcb212ca04696a02d1e08261faaf0",
      "869751c5aee14f1e9fb837bb3126e3a8",
      "5115453bcd3146da9200f2bcfc1edccb",
      "37a8d572047d416682e353ae96fbe67d",
      "c85daea927af45f5b7dceac7b5046f16",
      "a7c4eb9d9a124425af51fc56fb8b19c0",
      "4cb3e861d82141d485842d1ee071e5d5",
      "0f3510980f3e4331a1c9dcb4e991ef94",
      "2687bfa5b8b84e7a8fdf430278c0f034",
      "fd60ab7a9f4e4881bd31c1d72e1a8f92",
      "d255f96b9b3f4608968e322d104cdb6e",
      "5e822b0b0e6c4a35b3c3a05670ef596f",
      "39abf287751948038cf293ad971cd477",
      "1d196e11126b4048bc4f051ca7fc25a5",
      "9142db602b2f4968959891ddb0a16195"
     ]
    },
    "id": "2fGc3RcKbj5m",
    "outputId": "71202bb2-a03a-4f8b-b5df-5d85911f4fa7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_and_add_labels(data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "\n",
    "    unique_labels = set(entity[\"label\"] for sample in data for entity in sample.get(\"entities\", []))\n",
    "\n",
    "    label2id = {label: idx + 1 for idx, label in enumerate(unique_labels)}\n",
    "    label2id[\"[PAD]\"] = 0\n",
    "    tokenized_data = []\n",
    "\n",
    "    for sample in data:\n",
    "        text = sample.get(\"text\", \"\")\n",
    "        entities = sample.get(\"entities\", [])\n",
    "        text = text[:510]\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized_inputs[\"input_ids\"].squeeze().tolist()\n",
    "        split_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        labels = [0] * len(input_ids)\n",
    "        attention_mask = tokenized_inputs[\"attention_mask\"].squeeze().tolist()\n",
    "\n",
    "        for entity in entities:\n",
    "            start_offset = entity.get(\"start_offset\", 0)\n",
    "            end_offset = entity.get(\"end_offset\", 0)\n",
    "            label = entity.get(\"label\", \"\")\n",
    "\n",
    "            # Ensure indices are within the bounds of the text\n",
    "            start_offset = min(start_offset, len(text) - 1)\n",
    "            end_offset = min(end_offset, len(text) - 1)\n",
    "\n",
    "            start_token = tokenizer.encode(text[:start_offset], add_special_tokens=False)\n",
    "            end_token = tokenizer.encode(text[:end_offset], add_special_tokens=False)\n",
    "            if start_token and end_token:\n",
    "\n",
    "                start = len(start_token)\n",
    "                end = len(end_token)\n",
    "                label_id = label2id.get(label, 0)\n",
    "                labels[start:end + 1] = [label_id] * (end - start + 1)\n",
    "\n",
    "\n",
    "        tokenized_data.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"split_tokens\":split_tokens\n",
    "        })\n",
    "\n",
    "    return tokenized_data, label2id\n",
    "\n",
    "\n",
    "# Tokenize the data and add labels\n",
    "tokenized_data, label2id = tokenize_and_add_labels(merged_data)\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtStMJYEbj5o",
    "outputId": "46f47246-7ff4-4006-c172-221534d99fe2"
   },
   "outputs": [],
   "source": [
    "# Collect unique labels across all samples\n",
    "unique_labels = set(label for sample in tokenized_data for label in sample[\"labels\"])\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "label_list = list(label2id.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180,
     "referenced_widgets": [
      "0c3c3cf47dae407c9ecc1947f4e3d2d6",
      "5d21297c2a39488ab4f8d98ee2e4367c",
      "9bcfd4c1b6cc4c84a5f9130a578bed0f",
      "4b06d330c4974e11837801d812c8a601",
      "9a9ca22e884341c99300c2df4e61995e",
      "6b78d3b115b5489281aeea3d1af6e2e6",
      "3bb2b01335c540baa85605e9a756bb69",
      "ad2df5c671c54a26a6a57991e7a7e363",
      "ca5b6683e2b74c0f903659fa7a3b60d2",
      "357ed74c79f24e729febc74be0bca40b",
      "4b782c2c83394e61a0d359f5bf45d52b",
      "b3663b0fac47470aa5cb58e86cd0e8d5",
      "0dc0ce23767d45b6b008cbf8393f5b92",
      "dadb33f062614644860be39edff7922d",
      "af29ac65409a45bcb957bfc39f0d823f",
      "c1635f4b89174042b25b34533cec1faa",
      "29bbc00e93314370baf45fa44c6606ec",
      "e1d2f18061374c4aa782e4f824cf0590",
      "871207e4f2a94087a088d58fc3d77a99",
      "21a02fe31cc0426085775ae79058fb59",
      "06c4bf2f926e494ea5ea3c89f62e61b5",
      "53a63e36bbe04469ad4fb9d63d482aa9",
      "4edf9124e9f243d69bacbc6cabf4dffa",
      "d7b74c8bd46a4440ba6a574dcee36f6d",
      "a1d09313f5b94815aa95b69554321801",
      "9a8586b14e054191af56ef5f338fc58f",
      "bba083578fce45c0bb23ced296b781c5",
      "5df8955d78c84e14b7571245b9a8dafe",
      "727194d604ed43ebaa04d82b4e8ddc94",
      "38c3248615914d5eb28a167f71ec68e3",
      "92b29d9ca3594031a8ebec38d2735e81",
      "46a5585a6aa44a7ebb6f58cd96218c4e",
      "8edfbdf471764785b444d6233858b1e0",
      "f7c946217dd94c21881f0c89ecc16c84",
      "71408f02ef884f9ebd1a67116b3bb4e7",
      "724c4905334646dbb8bbc5ac7d67cae0",
      "015af444f3754a269ab9561decbcadff",
      "854825ad484e4ad2a3c276d63ebdbab7",
      "1d27e7abb761453abf60358db105ab42",
      "6e89a084325440349617d7fd6a46d43c",
      "849f542fe2804575a028437cb3d4edb5",
      "8a1777a9cbf8459297abfae6f9aebc61",
      "07f423966bcf49bf8df01779981e22bd",
      "a5a8d0a6ad0547d4996dc5057f6f0095"
     ]
    },
    "id": "W9X48NP_bj5o",
    "outputId": "0b3d3ee7-123e-4609-d4bc-5b8452e899ed"
   },
   "outputs": [],
   "source": [
    "# Hugging Face model references for Transformer library\n",
    "models = dict(\n",
    "    ROBERTA = \"roberta-base\",\n",
    "    DISTILBERT_U = \"distilbert-base-uncased\",\n",
    "    DISTILBERT_C = \"distilbert-base-cased\",\n",
    "    DEBERTA_V2_XL = \"microsoft/deberta-v2-xlarge\",\n",
    "    DEBERTA_V2_XXL = \"microsoft/deberta-v2-xxlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nb2SC42Pbj5o"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(tokenized_data, test_size=0.2)\n",
    "\n",
    "num_labels = len(label2id)\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Lhk_uos5bj5p"
   },
   "outputs": [],
   "source": [
    "for sample in tokenized_data:\n",
    "    if len(sample[\"labels\"]) != 512:\n",
    "        print(len(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UVU5oXWjmR-",
    "outputId": "29c493fb-8df1-47b6-bc7a-1fd0296405d5"
   },
   "outputs": [],
   "source": [
    "import os, re, math, random, json, string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import wandb\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import TrainerCallback, AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import DataCollatorForTokenClassification, PreTrainedModel, RobertaTokenizerFast\n",
    "\n",
    "from datasets import load_dataset, ClassLabel, Sequence, load_metric\n",
    "\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bPNOF6_qbj5q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstagarwal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4s-pWXuLbj5q",
    "outputId": "4372f978-8549-4b30-8fb2-1967ae4881d6"
   },
   "outputs": [],
   "source": [
    "# Logging date for w&b\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "log_date = today.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CBuEvGSrbj5r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT='P2D-NER-2021'\n",
      "env: WANDB_LOG_MODEL=false\n"
     ]
    }
   ],
   "source": [
    "# LOAD OR TRAIN MODEL\n",
    "TRAIN = 1 # 1 to TRAIN WEIGHTS or 0 to LOAD WEIGHTS\n",
    "\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "TRAIN_SPLIT = 0.90\n",
    "\n",
    "# RANDOM SEED FOR REPRODUCIBILITY\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# BATCH SIZE\n",
    "# TRY 4, 8, 16, 32, 64, 128, 256. REDUCE IF OOM ERROR, HIGHER FOR TPUS\n",
    "BATCH_SIZES = 1\n",
    "\n",
    "# EPOCHS - TRANSFORMERS ARE TYPICALLY FINE-TUNED BETWEEN 1 AND 3 EPOCHS \n",
    "EPOCHS = 10\n",
    "\n",
    "# WHICH PRE-TRAINED TRANSFORMER TO FINE-TUNE?\n",
    "MODEL_CHECKPOINT = models['DEBERTA_V2_XL']\n",
    "\n",
    "# SPECIFY THE WEIGHTS AND BIASES PROJECT NAME\n",
    "%env WANDB_PROJECT = 'P2D-NER-2021' \n",
    "\n",
    "# DETERMINE WHETHER TO SAVE THE MODEL IN THE 100GB OF FREE W&B STORAGE\n",
    "%env WANDB_LOG_MODEL = false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "626c26efee6a455888684b5154514059",
      "50ef4fed2efc4be78a3322c0375dde4e",
      "080391f86afd4b91b6045769b1ef787f",
      "e06c3b661348403fb334f2b14fbce91b",
      "3ff293e83f9c4456bbecb41576b6507e",
      "25459c82dfb441f9993faae9f7271713",
      "23a399e2905e46fd95ae90b5e3bd1044",
      "7b84c0cc3d5d4843a9db48d6e436a7bd",
      "ccad31bef1b547a6a348841bcc8c1ef4",
      "fa0d808f40cb41ff8c84a7211b27de0c",
      "e23a5683e360466ebd664e59d504293b"
     ]
    },
    "id": "MKpzMzh-pL0B",
    "outputId": "5427f1e5-cf49-4c39-ed3e-0921fa6333cc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits_flat = np.argmax(logits, axis=-1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return metric.compute(predictions=logits_flat, references=labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e9FCNcTXxZoL"
   },
   "outputs": [],
   "source": [
    "\n",
    "FEATURE_CLASS_LABELS = \"feature_class_labels.json\"\n",
    "DATA_FILE = 'all.json'\n",
    "TEMP_MODEL_OUTPUT_DIR = 'temp_model_output_dir'\n",
    "SAVED_MODEL = f\"p2d-NER-Fine-Tune-Transformer-{MODEL_CHECKPOINT}\" # Change for notebook version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "OK8AOfYMbj5r",
    "outputId": "6e74ff1d-e487-476b-f4cc-fbbf56a26070"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum learning rate is:  7.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Optimizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=26)\n",
    "learning_rate = 0.0000075\n",
    "lr_max = learning_rate * BATCH_SIZES\n",
    "weight_decay = 0.05\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr_max,\n",
    "    weight_decay=weight_decay)\n",
    "\n",
    "print(\"The maximum learning rate is: \",lr_max)\n",
    "\n",
    "# Learning Rate Schedule\n",
    "num_train_samples = len(train_data)\n",
    "warmup_ratio = 0.2 # Percentage of total steps to go from zero to max learning rate\n",
    "num_cycles=0.8 # The cosine exponential rate\n",
    "\n",
    "num_training_steps = num_train_samples*EPOCHS/BATCH_SIZES\n",
    "num_warmup_steps = num_training_steps*warmup_ratio\n",
    "\n",
    "lr_sched = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                           num_warmup_steps=num_warmup_steps,\n",
    "                                           num_training_steps = num_training_steps,\n",
    "                                           num_cycles=num_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "w2vhyQVsbj5r"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir = TEMP_MODEL_OUTPUT_DIR,\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         learning_rate=lr_max,\n",
    "                         per_device_train_batch_size=BATCH_SIZES,\n",
    "                         per_device_eval_batch_size=BATCH_SIZES,\n",
    "                         num_train_epochs=EPOCHS,\n",
    "                         weight_decay=weight_decay,\n",
    "                         lr_scheduler_type = 'cosine',\n",
    "                         warmup_ratio=warmup_ratio,\n",
    "                         logging_strategy=\"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         seed=RANDOM_SEED,\n",
    "                         report_to = 'wandb', # enable logging to W&B\n",
    "                         run_name = MODEL_CHECKPOINT+\"-\"+log_date,\n",
    "                         metric_for_best_model=\"f1\",\n",
    "                         load_best_model_at_end = True)   # name of the W&B run (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "txZhPh51s9G4"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6R7u5YB2oVf6"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    # Define the metric parameters\n",
    "    overall_precision = precision_score(true_labels, true_predictions, zero_division=1)\n",
    "    overall_recall = recall_score(true_labels, true_predictions, zero_division=1)\n",
    "    overall_f1 = f1_score(true_labels, true_predictions, zero_division=1)\n",
    "    overall_accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    \n",
    "    # Return a dictionary with the calculated metrics\n",
    "    return {\n",
    "        \"precision\": overall_precision,\n",
    "        \"recall\": overall_recall,\n",
    "        \"f1\": overall_f1,\n",
    "        \"accuracy\": overall_accuracy,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JkFxNdO7krNE"
   },
   "outputs": [],
   "source": [
    "# Define and instantiate the Trainer...\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=valid_data,\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics,\n",
    "                optimizers=(optimizer, lr_sched)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBqNPk53nl7w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/DocbasedQA/commercialClauses/wandb/run-20231212_200302-nr9xyj3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stagarwal/%27P2D-NER-2021%27/runs/nr9xyj3o' target=\"_blank\">microsoft/deberta-v2-xlarge-12-12-2023</a></strong> to <a href='https://wandb.ai/stagarwal/%27P2D-NER-2021%27' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stagarwal/%27P2D-NER-2021%27' target=\"_blank\">https://wandb.ai/stagarwal/%27P2D-NER-2021%27</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stagarwal/%27P2D-NER-2021%27/runs/nr9xyj3o' target=\"_blank\">https://wandb.ai/stagarwal/%27P2D-NER-2021%27/runs/nr9xyj3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='754' max='35980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  754/35980 06:31 < 5:05:37, 1.92 it/s, Epoch 0.21/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and instantiate\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "pred_trainer = Trainer(\n",
    "    loaded_model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predictions and produce a classification report\n",
    "predictions, labels, _ = pred_trainer.predict(valid_data)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "# Generate the metrics and display\n",
    "results = classification_report(true_labels, true_predictions, zero_division=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a sample for the evaluation set\n",
    "check = 3\n",
    "\n",
    "print(len(valid_data[check]['split_tokens']))\n",
    "print(len(true_predictions[check]))\n",
    "print(len(true_labels[check]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the predicted extracted data\n",
    "check_pred = zip(valid_data[check]['split_tokens'], true_predictions[check])\n",
    "for tup in check_pred:\n",
    "    if tup[1] != 'O':\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
